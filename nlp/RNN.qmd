---
title: "RNN (Recurrent Neural Network)"
description: "Text generation with RNN"
author: "ujjal"
date: "6/27/2023"
categories:
  - RNN
  - python
---

## RNN (Recurrent Neural Network)
- A RNN is a type of artificial neural network which uses sequential data or time series data
- An RNN is a several copy of same neural network that are aligned togther and each one passes its output to the next one
- Each copy call timestep because it receives different input at different time step 
- RNN are call recurrent because they perform the same task for every element of a sequence.
- RNN have a memory which capture information about what has been calculated so far
#### Application
- Rnn are used for time series analysis
- when we need to make a predction based on previous data, not only current data

#### Problems of standard network:
- Inputs, outputs can be different lengths in different examples.
- Doesnâ€™t share features learned across different positions of text. 

#### Types of RNN:
- one-to-one
- one-to-many
- many-to-one
- many-to-many
<img src='https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/cdp/cf/ul/g/ba/82/types-of-recurrent-neural-networks-combined.component.simple-narrative-l.ts=1681483684972.jpg/content/adobe-cms/us/en/topics/recurrent-neural-networks/jcr:content/root/table_of_contents/body/simple_narrative_1474352927/image'>




```python

```


```python

```


```python
import torch
import torch.nn as nn
import numpy as np
```


```python
class DictnaryEmbd(object):
    def __init__(self):
        self.idx = 0
        self.idx2word = {}
        self.word2idx = {}
    def addword(self,word):
        if word not in self.word2idx:
            self.idx2word[self.idx] = word
            self.word2idx[word] = self.idx
            self.idx += 1
    def __len__(self):
        return len(self.word2idx)

class Textpreprocess(object):
    def __init__(self):
        self.dict_embd = DictnaryEmbd()
    def get_data(self,file_path,batch_size):
        len_word = 0
        with open(file_path,'r') as f:
            for line in f:
                words = line.split() + ['<eos>']
                for w in words:
                    self.dict_embd.addword(w)
                    len_word += 1
        tensor = torch.LongTensor(len_word)
        with open(file_path,'r') as f:
            ind = 0
            for line in f:
                words = line.split() + ['<eos>']
                for w in words:
                    tensor[ind] = self.dict_embd.word2idx[w]
#         print(tensor.shape)
        num_batches = tensor.shape[0]//batch_size
        tensor = tensor[:batch_size*num_batches]
        tensor= tensor.view(batch_size,-1)
        return tensor
```


```python
corpus = Textpreprocess()
tensor = corpus.get_data('alice.txt',20)
tensor.shape
```




    torch.Size([20, 1484])




```python

```


```python
embed_size = 128   
hidden_size = 1024 
num_layers = 1
num_epochs = 20
batch_size = 20
timesteps = 30
learning_rate = 0.002

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```
