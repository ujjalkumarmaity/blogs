[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blogs",
    "section": "",
    "text": "In this post we’ll take a look at LSTM (Long Short-Term Memory)."
  },
  {
    "objectID": "index.html#lstm-long-short-term-memory",
    "href": "index.html#lstm-long-short-term-memory",
    "title": "blogs",
    "section": "",
    "text": "In this post we’ll take a look at LSTM (Long Short-Term Memory)."
  },
  {
    "objectID": "index.html#rnn-recurrent-neural-network",
    "href": "index.html#rnn-recurrent-neural-network",
    "title": "blogs",
    "section": "RNN (Recurrent Neural Network)",
    "text": "RNN (Recurrent Neural Network)\nIn this post we’ll take a look at RNN (Recurrent Neural Network)."
  },
  {
    "objectID": "index.html#logistic-regression",
    "href": "index.html#logistic-regression",
    "title": "blogs",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIn this post we’ll take a look at logistic regression and implement using python from scratch"
  },
  {
    "objectID": "index.html#spacy-tutorial",
    "href": "index.html#spacy-tutorial",
    "title": "blogs",
    "section": "Spacy Tutorial",
    "text": "Spacy Tutorial\nIn this article we’ll understand basic feature of spacy."
  },
  {
    "objectID": "index.html#python-tutorial",
    "href": "index.html#python-tutorial",
    "title": "blogs",
    "section": "Python Tutorial",
    "text": "Python Tutorial\nIn this post we’ll go through Python Interview Questions"
  },
  {
    "objectID": "index_1.html#logistic-regression",
    "href": "index_1.html#logistic-regression",
    "title": "blogs",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIn this article we’ll implement logistic regression in python from scratch"
  },
  {
    "objectID": "index_1.html#into-to-spacy",
    "href": "index_1.html#into-to-spacy",
    "title": "blogs",
    "section": "Into To Spacy",
    "text": "Into To Spacy\nIn this article we’ll understand basic feature of spacy"
  },
  {
    "objectID": "ml/logistic-regression-from-scratch/logistic-regression-from-scratch.html",
    "href": "ml/logistic-regression-from-scratch/logistic-regression-from-scratch.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Logistic Regression\nEDA\nData Visulization\nLogistic Regression using Gradient Descent\nLogistic Regression using Mini-batch SGD\nLogistic Regression using SGD with momentum\nLogistic Regression using using sklearn"
  },
  {
    "objectID": "ml/logistic-regression-from-scratch/logistic-regression-from-scratch.html#logistic-regression",
    "href": "ml/logistic-regression-from-scratch/logistic-regression-from-scratch.html#logistic-regression",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a statistical algorithm used for binary classification.Logistic regression is a type of supervised learning\nGiven an input feature vector x , Here we want to recognize this feature vector belongs to class 0 ot class 1\n\\(\\hat{y}\\)= \\(p(y=1|x)\\) , here \\(0&lt;=\\hat{y} &lt;=1\\)\nHere \\(x\\) is feature vector. parameter - \\(w\\)  If assuming a linear relationship between the input features and target variable. then \\(\\hat{y} = x*w^T\\)  \\(xw^T\\) can be much bigger then 1 or can be negative. but here we want predicted output should be between -0 and 1.\nIn Logistic Regression we use sigmiod function \\(\\hat{y} = \\sigma(x*w^T)\\)\n\n\\(\\sigma(x) = {1\\over(1+e^{-x})}\\) if \\(x\\) is very large then \\(e^{-x}\\) close to 0, \\(\\sigma(x) = 1\\)  if \\(x\\) is very small then \\(e^{-x}\\) is huge number, \\(\\sigma(x) = 0\\) \n\nLoss function\nIn logistic regression loss function $L(y,) = (1/2)*(y-)^2 $ not work well. we use following loss function \\[L(y,\\hat{y}) = - y log(\\hat{y}) - (1-y) log(1 - \\hat{y})\n\\] if \\(y=1\\) then \\(L(y,\\hat{y}) = - y log(\\hat{y})\\) &lt;- that means we want \\(y log(\\hat{y})\\) as large as possible, &lt;- that means \\(\\hat{y}\\) will be large. So if y = 1 , then we want \\(\\hat{y}\\) as biggest as possible. if \\(y=0\\) then \\(L(y,\\hat{y}) = - (1-y) log(1-\\hat{y})\\) &lt;- that means we want \\(log(1-\\hat{y})\\) large, &lt;- that means \\(\\hat{y}\\) will be small.\n\n\ncost function\n\\[J(W) =(1/m) \\sum L(y,\\hat{y})\n\\]\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/logistic-regression/Social_Network_Ads.csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\ndf=pd.read_csv('/kaggle/input/logistic-regression/Social_Network_Ads.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\nUser ID\n\n\nGender\n\n\nAge\n\n\nEstimatedSalary\n\n\nPurchased\n\n\n\n\n\n\n0\n\n\n15624510\n\n\nMale\n\n\n19\n\n\n19000\n\n\n0\n\n\n\n\n1\n\n\n15810944\n\n\nMale\n\n\n35\n\n\n20000\n\n\n0\n\n\n\n\n2\n\n\n15668575\n\n\nFemale\n\n\n26\n\n\n43000\n\n\n0\n\n\n\n\n3\n\n\n15603246\n\n\nFemale\n\n\n27\n\n\n57000\n\n\n0\n\n\n\n\n4\n\n\n15804002\n\n\nMale\n\n\n19\n\n\n76000\n\n\n0"
  },
  {
    "objectID": "nlp/lstm/LSTM.html",
    "href": "nlp/lstm/LSTM.html",
    "title": "LSTM (Long Short-Term Memory)",
    "section": "",
    "text": "LSTM is a type of RNN architecture that is widely used for sequnce modeling task\nLSTM overcome RNN limitation(vanising gradient) by introducing a memory cell and three gating mechanisms.\nMemory cell in LSTM allows to store and access information over long sequence\nLSTMs use a series of gates which control how the information in a sequence of data comes into, is stored in and leaves the network. they are -\n\nforgot gate\ninput gate\noutput gate\n\n\n\n\n\nNLP task- named entity recognition, sentiment analysis, machine translation etc.\nSpeech Recognition - automatic speech recognition, speech-to-text conversion etc.\nTime Series Analysis and Forecasting - stock market prediction, weather forecasting etc.\n\n\n\n\n\n\nFirst step in the process is Forgot gate. This gate telling the LSTM how much information keep from previous state. Output of this gate is between 0 and 1. Output of this forgot gate multiply with previous LSTM output.  output of forgot gate is 0 implies -&gt; Forget all previous memory output of forgot gate is 1 implies -&gt; Keep all previous memory output of forgot gate is 0.5 implies -&gt; Keep some of previous memory\n\n\n\nGoal of this step is how much information take from new memory. This sigmoid layer is call input gate decides which values we’ll update.\noutput of forgot gate is 0 implies -&gt; Didn’t take anything from generated memory output of forgot gate is 1 implies -&gt; Take anything from generated memory output of forgot gate is 0.5 implies -&gt; Take partially from generated memory \nThe new memory network is a tanh activated neural network which has learned how to combine the previous hidden state and new input data to generate a ‘new memory update vector’. This vector essentially contains information from the new input data given the context from the previous hidden state\n\n\n\n\nThe output gate, deciding the new hidden state. - First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. - Then, we put the cell state through tanh and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n\n\n\n\n\n\nComputational Complexity\nTraining Time\nDifficulty in Hyperparameter Tuning\n\n\n\n\n\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/"
  },
  {
    "objectID": "nlp/lstm/LSTM.html#lstm-long-short-term-memory",
    "href": "nlp/lstm/LSTM.html#lstm-long-short-term-memory",
    "title": "LSTM (Long Short-Term Memory)",
    "section": "",
    "text": "LSTM is a type of RNN architecture that is widely used for sequnce modeling task\nLSTM overcome RNN limitation(vanising gradient) by introducing a memory cell and three gating mechanisms.\nMemory cell in LSTM allows to store and access information over long sequence\nLSTMs use a series of gates which control how the information in a sequence of data comes into, is stored in and leaves the network. they are -\n\nforgot gate\ninput gate\noutput gate\n\n\n\n\n\nNLP task- named entity recognition, sentiment analysis, machine translation etc.\nSpeech Recognition - automatic speech recognition, speech-to-text conversion etc.\nTime Series Analysis and Forecasting - stock market prediction, weather forecasting etc.\n\n\n\n\n\n\nFirst step in the process is Forgot gate. This gate telling the LSTM how much information keep from previous state. Output of this gate is between 0 and 1. Output of this forgot gate multiply with previous LSTM output.  output of forgot gate is 0 implies -&gt; Forget all previous memory output of forgot gate is 1 implies -&gt; Keep all previous memory output of forgot gate is 0.5 implies -&gt; Keep some of previous memory\n\n\n\nGoal of this step is how much information take from new memory. This sigmoid layer is call input gate decides which values we’ll update.\noutput of forgot gate is 0 implies -&gt; Didn’t take anything from generated memory output of forgot gate is 1 implies -&gt; Take anything from generated memory output of forgot gate is 0.5 implies -&gt; Take partially from generated memory \nThe new memory network is a tanh activated neural network which has learned how to combine the previous hidden state and new input data to generate a ‘new memory update vector’. This vector essentially contains information from the new input data given the context from the previous hidden state\n\n\n\n\nThe output gate, deciding the new hidden state. - First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. - Then, we put the cell state through tanh and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n\n\n\n\n\n\nComputational Complexity\nTraining Time\nDifficulty in Hyperparameter Tuning\n\n\n\n\n\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/"
  },
  {
    "objectID": "nlp/rnn/RNN.html",
    "href": "nlp/rnn/RNN.html",
    "title": "RNN (Recurrent Neural Network)",
    "section": "",
    "text": "A RNN is a type of artificial neural network which uses sequential data or time series data\nA RNN is a several copy of same neural network that are aligned togther and each one passes its output to the next one\nEach copy call timestep because it receives different input at different time step\nRNN are call recurrent because they perform the same task for every element of a sequence.\nRNN have a memory which capture information about what has been calculated so far #### Application\nRnn are used for time series analysis\nwhen we need to make a predction based on previous data, not only current data\n\n\n\n\nInputs, outputs can be different lengths in different examples.\nDoesn’t share features learned across different positions of text.\n\n\n\n\n\none-to-one\none-to-many\nmany-to-one\nmany-to-many \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe BPTT algorithm is used to calculate the gradients of the loss with respect to the parameters of the RNN \nfirst calculate gradient with respect to (\\(w_0\\))\n\\[\\frac{\\mathrm{d}E_1}{\\mathrm{d}W_0} = \\frac{\\mathrm{d}E_1}{\\mathrm{d}y_1} \\frac{\\mathrm{d}y_1}{\\mathrm{d}W_0}\\] \\[\\frac{\\mathrm{d}E_2}{\\mathrm{d}W_0} = \\frac{\\mathrm{d}E_2}{\\mathrm{d}y_2} \\frac{\\mathrm{d}y_2}{\\mathrm{d}W_0}\\] \\[\\frac{\\mathrm{d}E_3}{\\mathrm{d}W_0} = \\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} \\frac{\\mathrm{d}y_3}{\\mathrm{d}W_0}\\] \\[\\frac{\\mathrm{d}E_4}{\\mathrm{d}W_0} = \\frac{\\mathrm{d}E_4}{\\mathrm{d}y_4} \\frac{\\mathrm{d}y_4}{\\mathrm{d}W_0}\\]\nAll RNN unit have same weights\ncalculate gradient with respect to recurrent weights(\\(w_h\\))\n\\[\\frac{\\mathrm{d}E_1}{\\mathrm{d}W_h} = \\frac{\\mathrm{d}E_1}{\\mathrm{d}y_1} \\frac{\\mathrm{d}y_1}{\\mathrm{d}h_1}  \\frac{\\mathrm{d}h_1}{\\mathrm{d}W_h}\\] \\[\\frac{\\mathrm{d}E_2}{\\mathrm{d}W_h} = \\frac{\\mathrm{d}E_2}{\\mathrm{d}y_2} \\frac{\\mathrm{d}y_2}{\\mathrm{d}h_2}  \\frac{\\mathrm{d}h_2}{\\mathrm{d}W_h}\\] we know in RNN current recurrent unit depedent on previous recurrent unit. so \\(h_2\\) depedent on \\(h_1\\). so the formula is- \\[\\frac{\\mathrm{d}E_2}{\\mathrm{d}W_h} =  \\frac{\\mathrm{d}E_2}{\\mathrm{d}y_2} \\frac{\\mathrm{d}y_2}{\\mathrm{d}h_2}  \\frac{\\mathrm{d}h_2}{\\mathrm{d}h_1} \\frac{\\mathrm{d}h_1}{\\mathrm{d}w_h} + \\frac{\\mathrm{d}E_2}{\\mathrm{d}y_2} \\frac{\\mathrm{d}y_2}{\\mathrm{d}h_2}  \\frac{\\mathrm{d}h_2}{\\mathrm{d}W_h}\\] similar \\[\\frac{\\mathrm{d}E_3}{\\mathrm{d}W_h} =  \\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} \\frac{\\mathrm{d}y_3}{\\mathrm{d}h_3}  \\frac{\\mathrm{d}h_3}{\\mathrm{d}w_h} + \\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} \\frac{\\mathrm{d}y_3}{\\mathrm{d}h_3}  \\frac{\\mathrm{d}h_3}{\\mathrm{d}h_2} \\frac{\\mathrm{d}h_2}{\\mathrm{d}w_h} +  \\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} \\frac{\\mathrm{d}y_3}{\\mathrm{d}h_3}  \\frac{\\mathrm{d}h_3}{\\mathrm{d}h_2} \\frac{\\mathrm{d}h_2}{\\mathrm{d}h_1} \\frac{\\mathrm{d}h_1}{\\mathrm{d}w_h}\\]\n\n\n\n\n\nThe output of the first RNN is passed to another RNN. Therefore the top-RNN receives the hidden state of the first RNN\n\n\n\n\n\nVanising Gradient - gradient can be very low. It refers to the issue where the gradients calculated during backpropagation become extremely small as they are propagated backward through time, leading to very slow or ineffective learning.\n\n\n\n\nDifficulty in Capturing Long-Term Dependencies: RNNs are designed to capture dependencies over long sequences. However, when the gradients vanish, the network struggles to propagate information over many time steps, limiting its ability to capture long-term dependencies\nSlow Learning: With vanishing gradients, the network learns at a slow pace since the weight updates in the early layers are negligible \\[\\frac{\\mathrm{d}E_3}{\\mathrm{d}W_h} =   \\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} \\frac{\\mathrm{d}y_3}{\\mathrm{d}h_3}  \\frac{\\mathrm{d}h_3}{\\mathrm{d}h_2} \\frac{\\mathrm{d}h_2}{\\mathrm{d}h_1} \\frac{\\mathrm{d}h_1}{\\mathrm{d}w_h} +....+\\] lets \\[\\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} = 0.3    \\frac{\\mathrm{d}y_3}{\\mathrm{d}h_3}=0.2    \\frac{\\mathrm{d}h_3}{\\mathrm{d}h_2} = 0.5   \\frac{\\mathrm{d}h_2}{\\mathrm{d}h_1} = 0.08    \\frac{\\mathrm{d}y_1}{\\mathrm{d}w_h}=0.01\\]\n\nSo \\(\\frac{\\mathrm{d}E_3}{\\mathrm{d}W_h} = 0.3*0.2*0.5*0.08*0.01 = 0.0002\\) almost 0\n\nExploding Gradient - Gradient can be very large , cause the loss and weiget update to heavily fluctuate \n\n\n\n\n\nGradient Clipping: clip the Gradient to spacific value the exceed a spacific threshold if gradients exceed to 5, then clip to 0.2\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nclass DictnaryEmbd(object):\n    def __init__(self):\n        self.idx = 0\n        self.idx2word = {}\n        self.word2idx = {}\n    def addword(self,word):\n        if word not in self.word2idx:\n            self.idx2word[self.idx] = word\n            self.word2idx[word] = self.idx\n            self.idx += 1\n    def __len__(self):\n        return len(self.word2idx)\n\nclass Textpreprocess(object):\n    def __init__(self):\n        self.dict_embd = DictnaryEmbd()\n    def get_data(self,file_path,batch_size):\n        len_word = 0\n        with open(file_path,'r') as f:\n            for line in f:\n                words = line.split() + ['&lt;eos&gt;']\n                for w in words:\n                    self.dict_embd.addword(w)\n                    len_word += 1\n        tensor = torch.LongTensor(len_word)\n        with open(file_path,'r') as f:\n            ind = 0\n            for line in f:\n                words = line.split() + ['&lt;eos&gt;']\n                for w in words:\n                    tensor[ind] = self.dict_embd.word2idx[w]\n#         print(tensor.shape)\n        num_batches = tensor.shape[0]//batch_size\n        tensor = tensor[:batch_size*num_batches]\n        tensor= tensor.view(batch_size,-1)\n        return tensor\ncorpus = Textpreprocess()\ntensor = corpus.get_data('alice.txt',20)\ntensor.shape\ntorch.Size([20, 1484])\n\nembed_size = 128   \nhidden_size = 1024 \nnum_layers = 1\nnum_epochs = 20\nbatch_size = 20\ntimesteps = 30\nlearning_rate = 0.002"
  },
  {
    "objectID": "nlp/rnn/RNN.html#rnn-recurrent-neural-network",
    "href": "nlp/rnn/RNN.html#rnn-recurrent-neural-network",
    "title": "RNN (Recurrent Neural Network)",
    "section": "",
    "text": "A RNN is a type of artificial neural network which uses sequential data or time series data\nA RNN is a several copy of same neural network that are aligned togther and each one passes its output to the next one\nEach copy call timestep because it receives different input at different time step\nRNN are call recurrent because they perform the same task for every element of a sequence.\nRNN have a memory which capture information about what has been calculated so far #### Application\nRnn are used for time series analysis\nwhen we need to make a predction based on previous data, not only current data\n\n\n\n\nInputs, outputs can be different lengths in different examples.\nDoesn’t share features learned across different positions of text.\n\n\n\n\n\none-to-one\none-to-many\nmany-to-one\nmany-to-many \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe BPTT algorithm is used to calculate the gradients of the loss with respect to the parameters of the RNN \nfirst calculate gradient with respect to (\\(w_0\\))\n\\[\\frac{\\mathrm{d}E_1}{\\mathrm{d}W_0} = \\frac{\\mathrm{d}E_1}{\\mathrm{d}y_1} \\frac{\\mathrm{d}y_1}{\\mathrm{d}W_0}\\] \\[\\frac{\\mathrm{d}E_2}{\\mathrm{d}W_0} = \\frac{\\mathrm{d}E_2}{\\mathrm{d}y_2} \\frac{\\mathrm{d}y_2}{\\mathrm{d}W_0}\\] \\[\\frac{\\mathrm{d}E_3}{\\mathrm{d}W_0} = \\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} \\frac{\\mathrm{d}y_3}{\\mathrm{d}W_0}\\] \\[\\frac{\\mathrm{d}E_4}{\\mathrm{d}W_0} = \\frac{\\mathrm{d}E_4}{\\mathrm{d}y_4} \\frac{\\mathrm{d}y_4}{\\mathrm{d}W_0}\\]\nAll RNN unit have same weights\ncalculate gradient with respect to recurrent weights(\\(w_h\\))\n\\[\\frac{\\mathrm{d}E_1}{\\mathrm{d}W_h} = \\frac{\\mathrm{d}E_1}{\\mathrm{d}y_1} \\frac{\\mathrm{d}y_1}{\\mathrm{d}h_1}  \\frac{\\mathrm{d}h_1}{\\mathrm{d}W_h}\\] \\[\\frac{\\mathrm{d}E_2}{\\mathrm{d}W_h} = \\frac{\\mathrm{d}E_2}{\\mathrm{d}y_2} \\frac{\\mathrm{d}y_2}{\\mathrm{d}h_2}  \\frac{\\mathrm{d}h_2}{\\mathrm{d}W_h}\\] we know in RNN current recurrent unit depedent on previous recurrent unit. so \\(h_2\\) depedent on \\(h_1\\). so the formula is- \\[\\frac{\\mathrm{d}E_2}{\\mathrm{d}W_h} =  \\frac{\\mathrm{d}E_2}{\\mathrm{d}y_2} \\frac{\\mathrm{d}y_2}{\\mathrm{d}h_2}  \\frac{\\mathrm{d}h_2}{\\mathrm{d}h_1} \\frac{\\mathrm{d}h_1}{\\mathrm{d}w_h} + \\frac{\\mathrm{d}E_2}{\\mathrm{d}y_2} \\frac{\\mathrm{d}y_2}{\\mathrm{d}h_2}  \\frac{\\mathrm{d}h_2}{\\mathrm{d}W_h}\\] similar \\[\\frac{\\mathrm{d}E_3}{\\mathrm{d}W_h} =  \\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} \\frac{\\mathrm{d}y_3}{\\mathrm{d}h_3}  \\frac{\\mathrm{d}h_3}{\\mathrm{d}w_h} + \\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} \\frac{\\mathrm{d}y_3}{\\mathrm{d}h_3}  \\frac{\\mathrm{d}h_3}{\\mathrm{d}h_2} \\frac{\\mathrm{d}h_2}{\\mathrm{d}w_h} +  \\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} \\frac{\\mathrm{d}y_3}{\\mathrm{d}h_3}  \\frac{\\mathrm{d}h_3}{\\mathrm{d}h_2} \\frac{\\mathrm{d}h_2}{\\mathrm{d}h_1} \\frac{\\mathrm{d}h_1}{\\mathrm{d}w_h}\\]\n\n\n\n\n\nThe output of the first RNN is passed to another RNN. Therefore the top-RNN receives the hidden state of the first RNN\n\n\n\n\n\nVanising Gradient - gradient can be very low. It refers to the issue where the gradients calculated during backpropagation become extremely small as they are propagated backward through time, leading to very slow or ineffective learning.\n\n\n\n\nDifficulty in Capturing Long-Term Dependencies: RNNs are designed to capture dependencies over long sequences. However, when the gradients vanish, the network struggles to propagate information over many time steps, limiting its ability to capture long-term dependencies\nSlow Learning: With vanishing gradients, the network learns at a slow pace since the weight updates in the early layers are negligible \\[\\frac{\\mathrm{d}E_3}{\\mathrm{d}W_h} =   \\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} \\frac{\\mathrm{d}y_3}{\\mathrm{d}h_3}  \\frac{\\mathrm{d}h_3}{\\mathrm{d}h_2} \\frac{\\mathrm{d}h_2}{\\mathrm{d}h_1} \\frac{\\mathrm{d}h_1}{\\mathrm{d}w_h} +....+\\] lets \\[\\frac{\\mathrm{d}E_3}{\\mathrm{d}y_3} = 0.3    \\frac{\\mathrm{d}y_3}{\\mathrm{d}h_3}=0.2    \\frac{\\mathrm{d}h_3}{\\mathrm{d}h_2} = 0.5   \\frac{\\mathrm{d}h_2}{\\mathrm{d}h_1} = 0.08    \\frac{\\mathrm{d}y_1}{\\mathrm{d}w_h}=0.01\\]\n\nSo \\(\\frac{\\mathrm{d}E_3}{\\mathrm{d}W_h} = 0.3*0.2*0.5*0.08*0.01 = 0.0002\\) almost 0\n\nExploding Gradient - Gradient can be very large , cause the loss and weiget update to heavily fluctuate \n\n\n\n\n\nGradient Clipping: clip the Gradient to spacific value the exceed a spacific threshold if gradients exceed to 5, then clip to 0.2\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nclass DictnaryEmbd(object):\n    def __init__(self):\n        self.idx = 0\n        self.idx2word = {}\n        self.word2idx = {}\n    def addword(self,word):\n        if word not in self.word2idx:\n            self.idx2word[self.idx] = word\n            self.word2idx[word] = self.idx\n            self.idx += 1\n    def __len__(self):\n        return len(self.word2idx)\n\nclass Textpreprocess(object):\n    def __init__(self):\n        self.dict_embd = DictnaryEmbd()\n    def get_data(self,file_path,batch_size):\n        len_word = 0\n        with open(file_path,'r') as f:\n            for line in f:\n                words = line.split() + ['&lt;eos&gt;']\n                for w in words:\n                    self.dict_embd.addword(w)\n                    len_word += 1\n        tensor = torch.LongTensor(len_word)\n        with open(file_path,'r') as f:\n            ind = 0\n            for line in f:\n                words = line.split() + ['&lt;eos&gt;']\n                for w in words:\n                    tensor[ind] = self.dict_embd.word2idx[w]\n#         print(tensor.shape)\n        num_batches = tensor.shape[0]//batch_size\n        tensor = tensor[:batch_size*num_batches]\n        tensor= tensor.view(batch_size,-1)\n        return tensor\ncorpus = Textpreprocess()\ntensor = corpus.get_data('alice.txt',20)\ntensor.shape\ntorch.Size([20, 1484])\n\nembed_size = 128   \nhidden_size = 1024 \nnum_layers = 1\nnum_epochs = 20\nbatch_size = 20\ntimesteps = 30\nlearning_rate = 0.002"
  },
  {
    "objectID": "nlp/spacy.html",
    "href": "nlp/spacy.html",
    "title": "Spacy",
    "section": "",
    "text": "spacy is open source library for NLP in python.\nSome Feature\n\nTokenization : split text into words, pancutation mark etc.\nPart-of-Speech Tagging : assign grammertical label to each token.\nNamed Entity Recognition (NER) - spacy can identify named entity in text. such as people name, ORG name\nDependency Parsing - Dependency parsing is the process of extracting the dependency parse of a sentence to represent its grammatical structure.\nLemmatization - transform word into root word is called a lemma\nSentence Boundary Detection(SBD) - finding and segmenting individual sentence\nEntity Linking (EL) -\nSimilarity - Comparing word, text span and document and how they similar to each other\nEule-based matching1 -\nTraining -\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")"
  },
  {
    "objectID": "nlp/spacy.html#spacy",
    "href": "nlp/spacy.html#spacy",
    "title": "Spacy",
    "section": "",
    "text": "spacy is open source library for NLP in python.\nSome Feature\n\nTokenization : split text into words, pancutation mark etc.\nPart-of-Speech Tagging : assign grammertical label to each token.\nNamed Entity Recognition (NER) - spacy can identify named entity in text. such as people name, ORG name\nDependency Parsing - Dependency parsing is the process of extracting the dependency parse of a sentence to represent its grammatical structure.\nLemmatization - transform word into root word is called a lemma\nSentence Boundary Detection(SBD) - finding and segmenting individual sentence\nEntity Linking (EL) -\nSimilarity - Comparing word, text span and document and how they similar to each other\nEule-based matching1 -\nTraining -\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")"
  },
  {
    "objectID": "nlp/spacy.html#tokenization",
    "href": "nlp/spacy.html#tokenization",
    "title": "Spacy",
    "section": "Tokenization",
    "text": "Tokenization\ntext = \"spaCy is an open-source software library for advanced natural language processing\"\nfor i in nlp(text):\n    print(i.text)\nspaCy\nis\nan\nopen\n-\nsource\nsoftware\nlibrary\nfor\nadvanced\nnatural\nlanguage\nprocessing"
  },
  {
    "objectID": "nlp/spacy.html#part-of-speech-tagging",
    "href": "nlp/spacy.html#part-of-speech-tagging",
    "title": "Spacy",
    "section": "Part-of-Speech Tagging",
    "text": "Part-of-Speech Tagging\nassign grammertical label to each token.\nhttps://universaldependencies.org/u/pos/\ntag_ represents the detailed part-of-speech tag assigned to the token, which includes more specific information about the word’s grammatical properties. EX - “NN” for a noun or “VB” for a verb.\npos_ it provides more general categorization of the word part of speech. ex - NOUN\nimport pandas as pd\ntext = \"spaCy is an open-source software library for advanced natural language processing\"\nl =[]\nfor token in nlp(text):\n    l.append([token.text,token.pos_,token.tag_,spacy.explain(token.tag_),token.dep_, token.shape_, token.is_alpha])\n    #print(token.text,'-&gt;',token.pos_,'-&gt;',token.tag_,spacy.explain(token.tag_),token.dep_, token.shape_, token.is_alpha)\npd.DataFrame(l,columns = ['tect','pos_','tag_','explain_tag','dep_','shape_','is_alpha'])    \n\n\n\n\n\n\n\n\n\ntect\n\n\npos_\n\n\ntag_\n\n\nexplain_tag\n\n\ndep_\n\n\nshape_\n\n\nis_alpha\n\n\n\n\n\n\n0\n\n\nspaCy\n\n\nINTJ\n\n\nUH\n\n\ninterjection\n\n\nnsubj\n\n\nxxxXx\n\n\nTrue\n\n\n\n\n1\n\n\nis\n\n\nAUX\n\n\nVBZ\n\n\nverb, 3rd person singular present\n\n\nROOT\n\n\nxx\n\n\nTrue\n\n\n\n\n2\n\n\nan\n\n\nDET\n\n\nDT\n\n\ndeterminer\n\n\ndet\n\n\nxx\n\n\nTrue\n\n\n\n\n3\n\n\nopen\n\n\nADJ\n\n\nJJ\n\n\nadjective (English), other noun-modifier (Chin…\n\n\namod\n\n\nxxxx\n\n\nTrue\n\n\n\n\n4\n\n\n-\n\n\nPUNCT\n\n\nHYPH\n\n\npunctuation mark, hyphen\n\n\npunct\n\n\n-\n\n\nFalse\n\n\n\n\n5\n\n\nsource\n\n\nNOUN\n\n\nNN\n\n\nnoun, singular or mass\n\n\ncompound\n\n\nxxxx\n\n\nTrue\n\n\n\n\n6\n\n\nsoftware\n\n\nNOUN\n\n\nNN\n\n\nnoun, singular or mass\n\n\ncompound\n\n\nxxxx\n\n\nTrue\n\n\n\n\n7\n\n\nlibrary\n\n\nNOUN\n\n\nNN\n\n\nnoun, singular or mass\n\n\nattr\n\n\nxxxx\n\n\nTrue\n\n\n\n\n8\n\n\nfor\n\n\nADP\n\n\nIN\n\n\nconjunction, subordinating or preposition\n\n\nprep\n\n\nxxx\n\n\nTrue\n\n\n\n\n9\n\n\nadvanced\n\n\nADJ\n\n\nJJ\n\n\nadjective (English), other noun-modifier (Chin…\n\n\namod\n\n\nxxxx\n\n\nTrue\n\n\n\n\n10\n\n\nnatural\n\n\nADJ\n\n\nJJ\n\n\nadjective (English), other noun-modifier (Chin…\n\n\namod\n\n\nxxxx\n\n\nTrue\n\n\n\n\n11\n\n\nlanguage\n\n\nNOUN\n\n\nNN\n\n\nnoun, singular or mass\n\n\ncompound\n\n\nxxxx\n\n\nTrue\n\n\n\n\n12\n\n\nprocessing\n\n\nNOUN\n\n\nNN\n\n\nnoun, singular or mass\n\n\npobj\n\n\nxxxx\n\n\nTrue"
  },
  {
    "objectID": "nlp/spacy.html#dependency-parsing",
    "href": "nlp/spacy.html#dependency-parsing",
    "title": "Spacy",
    "section": "Dependency Parsing",
    "text": "Dependency Parsing\nIt analyzes the syntactic structure of a sentence and assigns grammatical relationships between words, creating a parse tree representation\nfrom spacy import displacy\ntext = \"spaCy is an open-source software library for advanced natural language processing\"\ndoc = nlp(text)\noptions = {\"compact\":True, \"distance\":100, \"bg\":\"#0095b6\", \"color\":\"#DDDDDD\", \"font\":\"Source Sans Pro\"}\ndisplacy.render(doc, style='dep', jupyter=True,)\n  spaCy INTJ \n is AUX \n an DET \n open- ADJ \n source NOUN \n software NOUN \n library NOUN \n for ADP \n advanced ADJ \n natural ADJ \n language NOUN \n processing NOUN \n   nsubj   \n   det   \n   amod   \n   compound   \n   compound   \n   attr   \n   prep   \n   amod   \n   amod   \n   compound   \n   pobj"
  },
  {
    "objectID": "nlp/spacy.html#sentence-boundary-detectionsbd",
    "href": "nlp/spacy.html#sentence-boundary-detectionsbd",
    "title": "Spacy",
    "section": "Sentence Boundary Detection(SBD)",
    "text": "Sentence Boundary Detection(SBD)\nfinding start and end of the sentence\nsent = \"Processing raw text intelligently is difficult: most words are rare, and it’s common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. \"\ndoc = nlp(sent)\nprint(\"No of Sentence\" , len(list(doc.sents)))\nfor i in doc.sents:\n    print(i)\nNo of Sentence 3\nProcessing raw text intelligently is difficult: most words are rare, and it’s common for words that look completely different to mean almost the same thing.\nThe same words in a different order can mean something completely different.\nEven splitting text into useful word-like units can be difficult in many languages."
  },
  {
    "objectID": "nlp/spacy.html#lemmatization-stemming",
    "href": "nlp/spacy.html#lemmatization-stemming",
    "title": "Spacy",
    "section": "Lemmatization & Stemming",
    "text": "Lemmatization & Stemming\nLemmatization considers the context and converts the word to its meaningful base form, which is called Lemma\nStemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     C:\\Users\\ujjal\\AppData\\Roaming\\nltk_data...\n\n\n\n\n\nTrue\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\ntext = \"spaCy is an open-source software library for advanced natural language processing\"\nst = PorterStemmer()\nlema = WordNetLemmatizer()\nfor token in nlp(text):\n    print(token.text,'-&gt;',st.stem(token.text),lema.lemmatize(token.text),token.lemma_)\nspaCy -&gt; spaci spaCy spacy\nis -&gt; is is be\nan -&gt; an an an\nopen -&gt; open open open\n- -&gt; - - -\nsource -&gt; sourc source source\nsoftware -&gt; softwar software software\nlibrary -&gt; librari library library\nfor -&gt; for for for\nadvanced -&gt; advanc advanced advanced\nnatural -&gt; natur natural natural\nlanguage -&gt; languag language language\nprocessing -&gt; process processing processing"
  },
  {
    "objectID": "nlp/spacy.html#named-entity-recognition-ner",
    "href": "nlp/spacy.html#named-entity-recognition-ner",
    "title": "Spacy",
    "section": "Named Entity Recognition (NER)",
    "text": "Named Entity Recognition (NER)\nspaCy features an extremely fast statistical entity recognition system, that assigns labels to contiguous spans of tokens. The default trained pipelines can identify a variety of named and numeric entities, including companies, locations, organizations and products.\n\n\n\nimage.png\n\n\nsent = \"\"\"\nWe are looking for 8-10 years experienced DotNet Developer, JOB CODE - RDNLD2\n\nJob Description\n\n Candidates with 8+ years of experience in IT industry and with strong .Net/.Net\n\nCore/Azure Cloud Service/ Azure DevOps.\n\n Working hours – 8 hours , with a few hours of overlap during EST Time zone.\n\n Expertise in working with Microsoft Technologies like C#, .Net Core , WEB API,\n\nASP.Net MVC, ADO.Net, Entity Framework, IIS, Git\n\n Hands on experience in Azure development , worked on Azure Web application,\n\nApp Service, Azure Storage, Azure SQL database and Azure AD\n\n Expertise in Microsoft Azure Cloud Service , Application Insights, Azure\n\nMonitoring, KeyVault and SQL Azure.\n\n Hands on experience in building and deploying applications by adopting Azure\n\nDevOps practices such as Continuous Integration (CI) and Continuous\n\nDeployment (CD) in runtime with Git, Docker, Kubernetes and managing Azure\n\nCloud Services.\n\n Expertise in RDBMS including MS SQL Server with thorough knowledge in writing\n\nSQL queries, Stored Procedures, Views, Functions, Packages, Cursors &amp; tables\n\nand objects types.\n\n Good knowledge of JavaScript, React JS, jQuery, Angular,, and other languages\n\n Knowledge of architectural styles and design patterns, , experience in designing\n\nsolutions\n\n Broad and extensive knowledge of the software development life cycle (SDLC)\n\nwith software development models like Agile, Scrum model, Jira models.\n\n Designing, developing and executing software solutions to address business\n\nissues\n\n Strong debugging and problem-solving skills\n\n Manage, coordinate and support development team throughout the process\n\n Perform object-oriented programming, data modelling and database creation.\n\n Experience in Stakeholder Management (client facing roles)\n\n Excellent communication and organizational skills\n\n\n\n﻿Responsibilities\n\n Develop high-quality software design and architecture\n\n Identify, prioritize and execute tasks in the software development life cycle\n\n Guide team to write reusable, testable, performant and efficient code\n\n Work with the team to define, design, and deliver on new features\n\n Lead by example by designing and implementing clean best-practices\n\nmaintainable code\n\n Experience in large scale software development.\n\nPrimary Skills\n\n Expertise in C#, .Net Core, Entity framework, EF core, Microservices, Azure Cloud\n\nservices, Azure DevOps\n\n Expertise in RDBMS including MS SQL Server with thorough knowledge in writing\n\nSQL queries, Stored Procedures\n\nSoft Skills\n\n Team Management\n\n Communication Skills\n\n Documentation skills\n\n Leadership and ownership quality\n\n Mentor junior team members\n\n\n\n\"\"\"\n#label is hash value\n# sent = \"spaCy is an open-source software library for advanced natural language processing\"\n# sent = \"Apple is looking at buying San Francisco startup \"\ndoc = nlp(sent)\nfor ent in doc.ents:\n    print(ent.text,ent.label_,ent.start_char,ent.label)\n8-10 years DATE 20 391\nDotNet Developer ORG 43 383\n8+ years DATE 115 391\nWorking hours TIME 223 392\n8 hours TIME 239 392\na few hours TIME 254 392\nEST Time LOC 284 385\nExpertise GPE 302 384\nMicrosoft Technologies ORG 328 383\nMVC ORG 390 383\nEntity Framework PERSON 404 380\nGit PERSON 427 380\nAzure Storage ORG 524 383\nExpertise GPE 574 384\nMicrosoft Azure Cloud Service ORG 587 383\nKeyVault ORG 660 383\nSQL Azure ORG 673 383\nDevOps ORG 765 383\nContinuous Integration ORG 790 383\nCI ORG 814 383\nContinuous\n\nDeployment ORG 822 383\nGit GPE 866 384\nDocker GPE 871 384\nKubernetes ORG 879 383\nExpertise GPE 929 384\nMS SQL Server ORG 958 383\nSQL ORG 1008 383\nStored Procedures PERSON 1021 380\nViews ORG 1040 383\nFunctions PERSON 1047 380\nPackages, Cursors & ORG 1058 383\nJavaScript PRODUCT 1130 386\nBroad ORG 1288 383\nAgile ORG 1403 383\nJira PERSON 1423 380\nManage PRODUCT 1572 386\nStakeholder Management ORG 1738 383\nDevelop ORG 1860 383\nIdentify ORG 1917 383\nGuide ORG 1998 383\nPrimary Skills PERSON 2278 380\nExpertise GPE 2405 384\nMS SQL Server ORG 2434 383\nSQL ORG 2484 383\nSoft Skills PERSON 2516 380\nTeam Management ORG 2531 383\nDocumentation PRODUCT 2574 386\nLeadership NORP 2598 381\nMentor LOC 2634 385\nfrom spacy import displacy\ndisplacy.render(doc,style= \"ent\",jupyter=True )\n\n\nWe are looking for  8-10 years DATE  experienced  DotNet Developer ORG  , JOB CODE - RDNLD2Job Description Candidates with  8+ years DATE  of experience in IT industry and with strong .Net/.NetCore/Azure Cloud Service/ Azure DevOps.  Working hours TIME  –  8 hours TIME  , with  a few hours TIME  of overlap during  EST Time LOC  zone.  Expertise GPE  in working with  Microsoft Technologies ORG  like C#, .Net Core , WEB API,ASP.Net  MVC ORG  , ADO.Net,  Entity Framework PERSON  , IIS,  Git PERSON   Hands on experience in Azure development , worked on Azure Web application,App Service,  Azure Storage ORG  , Azure SQL database and Azure AD  Expertise GPE  in  Microsoft Azure Cloud Service ORG  , Application Insights, AzureMonitoring,  KeyVault ORG  and  SQL Azure ORG  . Hands on experience in building and deploying applications by adopting Azure  DevOps ORG  practices such as  Continuous Integration ORG  (  CI ORG  ) and  Continuous\nDeployment ORG  (CD) in runtime with  Git GPE  ,  Docker GPE  ,  Kubernetes ORG  and managing AzureCloud Services.  Expertise GPE  in RDBMS including  MS SQL Server ORG  with thorough knowledge in writing  SQL ORG  queries,  Stored Procedures PERSON  ,  Views ORG  ,  Functions PERSON  ,  Packages, Cursors & ORG  amp; tablesand objects types. Good knowledge of  JavaScript PRODUCT  , React JS, jQuery, Angular,, and other languages Knowledge of architectural styles and design patterns, , experience in designingsolutions  Broad ORG  and extensive knowledge of the software development life cycle (SDLC)with software development models like  Agile ORG  , Scrum model,  Jira PERSON  models. Designing, developing and executing software solutions to address businessissues Strong debugging and problem-solving skills  Manage PRODUCT  , coordinate and support development team throughout the process Perform object-oriented programming, data modelling and database creation. Experience in  Stakeholder Management ORG  (client facing roles) Excellent communication and organizational skills﻿Responsibilities  Develop ORG  high-quality software design and architecture  Identify ORG  , prioritize and execute tasks in the software development life cycle  Guide ORG  team to write reusable, testable, performant and efficient code Work with the team to define, design, and deliver on new features Lead by example by designing and implementing clean best-practicesmaintainable code Experience in large scale software development.  Primary Skills PERSON   Expertise in C#, .Net Core, Entity framework, EF core, Microservices, Azure Cloudservices, Azure DevOps  Expertise GPE  in RDBMS including  MS SQL Server ORG  with thorough knowledge in writing  SQL ORG  queries, Stored Procedures  Soft Skills PERSON    Team Management ORG   Communication Skills  Documentation PRODUCT  skills  Leadership NORP  and ownership quality  Mentor LOC  junior team members"
  },
  {
    "objectID": "python/python_advance.html",
    "href": "python/python_advance.html",
    "title": "Python Advance",
    "section": "",
    "text": "Lambda is a keyword in Python used to define functions, more specifically Anonymous Functions lambda arguments: expression\nsum_fun = lambda a,b : a+b\nsum_fun(2,3)\n5"
  },
  {
    "objectID": "python/python_advance.html#inheritance",
    "href": "python/python_advance.html#inheritance",
    "title": "Python Advance",
    "section": "Inheritance",
    "text": "Inheritance\nInheritance allows us to define a class that inherits all the methods and properties from another class.\nParent class is the class being inherited from, also called base class.\nChild class is the class that inherits from another class, also called derived class.\nWhen you add the __init__() function, the child class will no longer inherit the parent’s __init__() function.\nclass Student(Person):\n  def __init__(self, fname, lname):\nTo keep the inheritance of the parent’s __init__() function, add a call to the parent’s __init__() function:\nclass Student(Person):\n  def __init__(self, fname, lname):\n    Person.__init__(self, fname, lname)\nsuper() function that will make the child class inherit all the methods and properties from its parent\nclass Student(Person):\n  def __init__(self, fname, lname):\n    super().__init__(fname, lname)\nclass Person:\n    def __init__(self, fname, lname):\n        self.firstname = fname\n        self.lastname = lname\n\n    def printname(self):\n        print(self.firstname, self.lastname)\n\nclass Student(Person):\n    def __init__(self, fname, lname, year):\n        super().__init__(fname, lname)\n        self.graduationyear = year\n\n    def printstd(self):\n        print(\"Welcome\", self.firstname, self.lastname, \"to the class of\", self.graduationyear)\n\nx = Student(\"Mike\", \"Olsen\", 2023)\nx.printstd()\nWelcome Mike Olsen to the class of 2023"
  },
  {
    "objectID": "my.html",
    "href": "my.html",
    "title": "IDEA",
    "section": "",
    "text": "LSTM IMPLEMENT SCRATCH\nCATEGORY PREDCTION MODEL\nKAGGLE COMPITION\nCOMPLETE RESEARCH READ"
  }
]