[
  {
    "objectID": "2022-12-28-Mean-Median-Imputation.html",
    "href": "2022-12-28-Mean-Median-Imputation.html",
    "title": "Matplotlib Demo",
    "section": "",
    "text": "Mean & Median Imputation\n\nAll about Mean & Median Imputation.\n\n\ntoc: true\nbadges: true\ncomments: true\ncategories: [jupyter]\n\nWhen we can use - Data is missing completly random - No more the 5% of the variable contain missing\nMean/Median Imputation Assumptions - Data is missing at random\nMean/Median Imputation Advantage - Easy to implement - Fast way of obtaining in complete dataset\nMean/Median Imputation Limitation - Distortion of the original variable distrubtion & variance - Higher percentage of NA, the higher distortion\nImprotant Note - If variable is normal distributed then mean imutation is better - If variable is skewed then median imutation is better - Imputation value should be calculated in training set and same value impute in test set\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('titanic.csv')\ndf.head(2)\n\n\n\n\n\n\n\n\n\nPassengerId\n\n\nSurvived\n\n\nPclass\n\n\nName\n\n\nSex\n\n\nAge\n\n\nSibSp\n\n\nParch\n\n\nTicket\n\n\nFare\n\n\nCabin\n\n\nEmbarked\n\n\n\n\n\n\n0\n\n\n1\n\n\n0\n\n\n3\n\n\nBraund, Mr. Owen Harris\n\n\nmale\n\n\n22.0\n\n\n1\n\n\n0\n\n\nA/5 21171\n\n\n7.2500\n\n\nNaN\n\n\nS\n\n\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\nCumings, Mrs. John Bradley (Florence Briggs Th…\n\n\nfemale\n\n\n38.0\n\n\n1\n\n\n0\n\n\nPC 17599\n\n\n71.2833\n\n\nC85\n\n\nC\n\n\n\n\n\n\ndf.isnull().sum()\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\ndf.isnull().mean()\nPassengerId    0.000000\nSurvived       0.000000\nPclass         0.000000\nName           0.000000\nSex            0.000000\nAge            0.198653\nSibSp          0.000000\nParch          0.000000\nTicket         0.000000\nFare           0.000000\nCabin          0.771044\nEmbarked       0.002245\ndtype: float64\n#mean\nprint(df.Age.mean())\ndf['Age_mean_imput'] = df.Age.fillna(df.Age.mean())\n29.69911764705882\n#median\nprint(df.Age.median())\ndf['Age_median_imput'] = df.Age.fillna(df.Age.median())\n28.0\ndf[['Age','Age_mean_imput','Age_median_imput']].tail(5)\n\n\n\n\n\n\n\n\n\nAge\n\n\nAge_mean_imput\n\n\nAge_median_imput\n\n\n\n\n\n\n886\n\n\n27.0\n\n\n27.000000\n\n\n27.0\n\n\n\n\n887\n\n\n19.0\n\n\n19.000000\n\n\n19.0\n\n\n\n\n888\n\n\nNaN\n\n\n29.699118\n\n\n28.0\n\n\n\n\n889\n\n\n26.0\n\n\n26.000000\n\n\n26.0\n\n\n\n\n890\n\n\n32.0\n\n\n32.000000\n\n\n32.0\n\n\n\n\n\n\ndf[['Age','Age_mean_imput','Age_median_imput']].describe()\n\n\n\n\n\n\n\n\n\nAge\n\n\nAge_mean_imput\n\n\nAge_median_imput\n\n\n\n\n\n\ncount\n\n\n714.000000\n\n\n891.000000\n\n\n891.000000\n\n\n\n\nmean\n\n\n29.699118\n\n\n29.699118\n\n\n29.361582\n\n\n\n\nstd\n\n\n14.526497\n\n\n13.002015\n\n\n13.019697\n\n\n\n\nmin\n\n\n0.420000\n\n\n0.420000\n\n\n0.420000\n\n\n\n\n25%\n\n\n20.125000\n\n\n22.000000\n\n\n22.000000\n\n\n\n\n50%\n\n\n28.000000\n\n\n29.699118\n\n\n28.000000\n\n\n\n\n75%\n\n\n38.000000\n\n\n35.000000\n\n\n35.000000\n\n\n\n\nmax\n\n\n80.000000\n\n\n80.000000\n\n\n80.000000\n\n\n\n\n\n\n#Variance\nprint(\"variance of original variable\",df.Age.var())\nprint(\"variance after mean imputation\",df.Age_mean_imput.var())\nprint(\"variance after median imputation\",df.Age_median_imput.var())\nvariance of original variable 211.0191247463081\nvariance after mean imputation 169.05239993721085\nvariance after median imputation 169.51249827942328\nax = df.Age.plot(kind='kde')\nax.legend()\nax = df.Age_mean_imput.plot(kind='kde',color='green')\nax.legend()\nax = df.Age_median_imput.plot(kind='kde',color='red')\nax.legend()\n&lt;matplotlib.legend.Legend at 0x2151dc16340&gt;\n\n\n\npng\n\n\nHere we can see distortion of mean/median imputation is higher than original variable\nmean/median imputaion may affect relationship between other variable\ndf[['Fare','Age','Age_mean_imput','Age_median_imput']].cov()\n\n\n\n\n\n\n\n\n\nFare\n\n\nAge\n\n\nAge_mean_imput\n\n\nAge_median_imput\n\n\n\n\n\n\nFare\n\n\n2469.436846\n\n\n73.849030\n\n\n59.162200\n\n\n62.556767\n\n\n\n\nAge\n\n\n73.849030\n\n\n211.019125\n\n\n211.019125\n\n\n211.019125\n\n\n\n\nAge_mean_imput\n\n\n59.162200\n\n\n211.019125\n\n\n169.052400\n\n\n169.052400\n\n\n\n\nAge_median_imput\n\n\n62.556767\n\n\n211.019125\n\n\n169.052400\n\n\n169.512498\n\n\n\n\n\n\ndf[['Age','Age_mean_imput','Age_median_imput']].plot(kind='box')\n&lt;AxesSubplot:&gt;\n\n\n\npng\n\n\nafter mean/median imputation there are more outlier."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blogs",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "ml/2021-12-28-logistic-regression-from-scratch.html",
    "href": "ml/2021-12-28-logistic-regression-from-scratch.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Logistic Regression Using Scratch\n\nA tutorial of Logistic Regression Using Scratch\n\n\ntoc: true\nbadges: true\ncomments: true\ncategories: [jupyter]\n\n\nTable of content\n\nEDA\nData Visulization\nLogistic Regression using Gradient Descent\nLogistic Regression using Mini-batch SGD\nLogistic Regression using SGD with momentum\nLogistic Regression using using sklearn\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\n\n\n\nCode\ndf=pd.read_csv('/kaggle/input/logistic-regression/Social_Network_Ads.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15624510\nMale\n19\n19000\n0\n\n\n1\n15810944\nMale\n35\n20000\n0\n\n\n2\n15668575\nFemale\n26\n43000\n0\n\n\n3\n15603246\nFemale\n27\n57000\n0\n\n\n4\n15804002\nMale\n19\n76000\n0\n\n\n\n\n\n\n\n\n\n\nEDA\n\n\nCode\n# Drop User id\nlen(df['User ID'].unique())\ndf.drop(columns=['User ID'],inplace=True)\n\n\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\ncount\n400.000000\n400.000000\n400.000000\n\n\nmean\n37.655000\n69742.500000\n0.357500\n\n\nstd\n10.482877\n34096.960282\n0.479864\n\n\nmin\n18.000000\n15000.000000\n0.000000\n\n\n25%\n29.750000\n43000.000000\n0.000000\n\n\n50%\n37.000000\n70000.000000\n0.000000\n\n\n75%\n46.000000\n88000.000000\n1.000000\n\n\nmax\n60.000000\n150000.000000\n1.000000\n\n\n\n\n\n\n\n\n\nCode\ndf.isnull().sum()\n\n\nGender             0\nAge                0\nEstimatedSalary    0\nPurchased          0\ndtype: int64\n\n\n\n\nCode\ndf.dtypes\n\n\nGender             object\nAge                 int64\nEstimatedSalary     int64\nPurchased           int64\ndtype: object\n\n\n\n\nCode\n#conert categorical feature to numarical feature\nle=LabelEncoder()\ndf['Gender']=le.fit_transform(df['Gender'])\n\n\n\n\nCode\n#Normalize the data\nsc=MinMaxScaler()\ndf_n=sc.fit_transform(df.iloc[:,:-1])\n\n\n\n\nCode\n#train test split\nx_train,x_test,y_train,y_test=train_test_split(df_n,df['Purchased'])\ny_train.reset_index(drop=True,inplace=True)\ny_test.reset_index(drop=True,inplace=True)\nx=x_train\ny=y_train\n\n\n\n\nData Visulization\n\n\nCode\n#pairplot\nsns.pairplot(df,hue='Purchased')\n\n\n\n\n\n\n\nCode\nsns.boxplot(x='Purchased',y='EstimatedSalary',data=df)\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd936293d10&gt;\n\n\n\n\n\n\n\nCode\nsns.boxplot(x='Purchased',y='Age',data=df)\n\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd934a0a790&gt;\n\n\n\n\n\n\n\nCode\n#pie plot\ndf_gender=df[['Gender','Purchased']].groupby('Purchased').sum()\ndf_gender.index=['Male','Female']\ndf_gender['Gender'].plot(kind='pie',autopct='%1.1f%%')\nplt.show()\n\n\n\n\n\n\n\nLogistic Regression using Gradient Descent \n\n\nCode\ndef sigmoid(x,w,b):\n    return 1/(1+np.exp(-(np.dot(x,w)+b)))\ndef loss(x,w,y,b):\n    s=sigmoid(x,w,b)\n    return np.mean(-(y*np.log(s))- ((1-y)*np.log(1-s)))\ndef grad(x,y,w,b):\n    s=sigmoid(x,w,b)    \n    return np.dot(x.T,(s-y))/x.shape[0]\n\n\n\n\nCode\ndef accuracy(y_pred,y_test):\n    return np.mean(y_pred==y_test)\n\n\n\n\nCode\n# initilize w and b\ndef gradientdescent(x,y):\n    w=np.zeros((x.shape[1]))\n    b=np.zeros(1)\n    ite=1000 #number of iteration\n    eta=0.7 #learning rate\n    loss_v=[]\n    for i in range(ite):\n        probability=sigmoid(x,w,b)\n        l=loss(x,w,y,b)\n        gradient=grad(x,y,w,b)\n        w=w- (eta*gradient)\n        b=b-(eta*np.sum(probability-y)/x.shape[0])\n        loss_v.append(l)\n        if i%100==0:\n            print(l)\n    return w,b,loss_v\n\n\n\n\nCode\nw,b,loss_v=gradientdescent(x,y)\ny_pred=sigmoid(x_test,w,b)\nfor j,i in enumerate(y_pred):\n    if i&lt;0.5:\n        y_pred[j]=0\n    else:\n        y_pred[j]=1\n\nprint('test accuracy',accuracy(y_pred,y_test))\n\n\n0.6931471805599467\n0.46824620053813504\n0.41373079197199336\n0.3897267439098201\n0.37674477979951454\n0.36885655071698165\n0.363696412749435\n0.36014577026616207\n0.3576113857482108\n0.35575160674492456\ntest accuracy 0.86\n\n\n\n\nCode\nplt.plot(range(len(loss_v)),loss_v)\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\n\n\n\nLogistic Regression using Mini-batch SGD\n\n\nCode\nbatch_size=8\ndef sgd(x,y,batch_size):\n    # initilize w and b\n    w=np.zeros((x_train.shape[1]))\n    b=np.zeros(1)\n    ite=1000 #number of iteration\n    eta=0.7 #learning rate\n    loss_v=[]\n    for i in range(1000):\n        ind=np.random.choice(len(y_train),batch_size)\n        x_b=x[ind]\n        y_b=y[ind]\n        p=sigmoid(x_b,w,b)\n        l=loss(x_b,w,y_b,b)\n        gradient=grad(x_b,y_b,w,b)\n        w=w- (0.1*gradient)\n        b=b-(eta*np.sum(p-y_b)/x.shape[0])\n        if i%10==0:\n            loss_v.append(l)\n        if i%100==0:\n            print('loss',l)\n    return w,b,loss_v\n\n\n\n\nCode\nw,b,loss_v=sgd(x,y,32)\ny_pred=sigmoid(x_test,w,b)\nfor j,i in enumerate(y_pred):\n    if i&lt;0.5:\n        y_pred[j]=0\n    else:\n        y_pred[j]=1\n\nprint('test accuracy',accuracy(y_pred,y_test))\n\n\nloss 0.6931471805599448\nloss 0.6278149588111854\nloss 0.6035356489914048\nloss 0.4881741340927539\nloss 0.5486975396008116\nloss 0.4963472981460031\nloss 0.4807055091535177\nloss 0.5649417248839724\nloss 0.4608513419074556\nloss 0.5171429870812208\ntest accuracy 0.84\n\n\n\n\nCode\nplt.plot(range(len(loss_v)),loss_v)\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\n\n\n\nLogistic Regression using SGD with momentum\n\n\nCode\nbatch_size=8\ndef sgdmomentum(x,y,batch_size):\n    # initilize w and b\n    w=np.zeros((x_train.shape[1]))\n    b=np.zeros(1)\n    ite=1000 #number of iteration\n    eta=0.7 #learning rate\n    alpha=0.9\n    loss_v=[]\n    v_t=np.zeros((x_train.shape[1])) \n    v_b=np.zeros(1)\n    for i in range(1000):\n        ind=np.random.choice(len(y_train),batch_size)\n        x_b=x[ind]\n        y_b=y[ind]\n        p=sigmoid(x_b,w,b)\n        l=loss(x_b,w,y_b,b)\n        gradient=grad(x_b,y_b,w,b)\n        v_t =(alpha*v_t) + (eta*gradient)\n        w=w-v_t\n        v_b=(alpha*v_b) + (eta*np.sum(p-y_b)/x.shape[0])\n        b=b-v_b\n        if i%10==0:\n            loss_v.append(l)\n        if i%100==0:\n            print('loss',l)\n    return w,b,loss_v\n\n\n\n\nCode\nw,b,loss_v=sgdmomentum(x,y,32)\n\n\nloss 0.6931471805599448\nloss 0.4220835670845099\nloss 0.2941736243371927\nloss 0.44537673992679633\nloss 0.2871349895011394\nloss 0.6241278912840013\nloss 0.34683687828696796\nloss 0.18828219280440267\nloss 0.4223695477823046\nloss 0.34499265763927867\n\n\n\n\nCode\nplt.plot(range(len(loss_v)),loss_v)\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\n\n\n\nCode\n#Predction\ny_pred=sigmoid(x_test,w,b)\nfor j,i in enumerate(y_pred):\n    if i&lt;0.5:\n        y_pred[j]=0\n    else:\n        y_pred[j]=1\n\nprint('test accuracy',accuracy(y_pred,y_test))\n\n\ntest accuracy 0.86\n\n\n\n\nLogistic Regression using Using sklearn\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\n\n\n\n\nCode\nmodel= LogisticRegression()\nmodel.fit(x_train,y_train)\ny_pred=model.predict(x_test)\nprint('test accuracy',accuracy(y_pred,y_test))\n\n\ntest accuracy 0.82"
  }
]